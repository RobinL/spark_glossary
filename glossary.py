import json

glossary = {}

def add_to_glossary(key, short_definition, definition):
  glossary[key] = {
    "short_definition": short_definition.strip(),
    "definition": definition.strip(),
    "key": key
  }

# Exchange
short_definition = """
An exchange is shorthand for a 'shuffle exchange', often described merely as a 'shuffle'.
"""

definition = """
'Exchange' is shorthand for a 'Shuffle Exchange. This means that Spark is sending intermediate data between executors, probably across the network. This is more often described as merely a 'shuffle'.  This happens between jobs.
"""

add_to_glossary("exchange", short_definition, definition)

# Project
short_definition = """
Picking a subset of available columns
"""

definition = """
This 'can be roughly thought of as picking a subset of all available columns. For example, if the attributes are (name, age), then projection of the relation {(Alice, 5), (Bob, 8)} onto attribute list (age) yields {5,8}"
"""

add_to_glossary("project", short_definition, definition)

# Shuffle
short_definition = """
A procedure for spark executors either in same physical node or in different physical nodes to exchange intermedia data generated by map tasks and required by reduce tasks.
"""

definition = """
A procedure for spark executors either in same physical node or in different physical nodes to exchange intermedia data generated by map tasks and required by reduce tasks.  See [here]
(https://xuechendi.github.io/2019/04/15/Spark-Shuffle-and-Spill-Explained)"""

add_to_glossary("shuffle", short_definition, definition)


# Stage
short_definition = "Jobs are divided into 'stages' based on the shuffle boundary"

definition = """
Jobs are divided into "stages" based on the shuffle boundary.
A stage is a collection of tasks that run the same code, each on a different subset of the data. See [here](https://stackoverflow.com/a/42266758/1779128) and [here](https://docs.cloudera.com/documentation/enterprise/5-6-x/topics/cdh_ig_spark_apps.html).
"""

add_to_glossary("stage", short_definition, definition)

# Job
short_definition = """
A job is created when you apply an action on an RDD.  There is a one to one correspondence between actions and jobs.
"""

definition = """
Jobs are work which is submitted to Spark. A job is created when you apply an action on an RDD.    See [here](https://intellipaat.com/community/18528/what-is-the-concept-of-application-job-stage-and-task-in-spark)
"""

add_to_glossary("job", short_definition, definition)

# WholeStageCodeGen

short_definition = """
An optimisation in Spark where several map operations are combined into a single step
"""

definition = """
WholeStageCodeGen is an optimisation in Spark where several map operations are combined into a single step of the code that Spark generates.  You can think of this as two lambda funnctions (steps) being combined into a single lambda function for execution.
"""

with open("glossary.json", "w") as f:
  json.dump(glossary, f, indent=4)
